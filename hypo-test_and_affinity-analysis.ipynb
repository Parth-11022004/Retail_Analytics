{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f201b75",
   "metadata": {},
   "source": [
    "## Hypothesis testing and affinity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92857427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADATE</th>\n",
       "      <th>STORE_NBR</th>\n",
       "      <th>LYLTY_CARD_NBR</th>\n",
       "      <th>TXN_ID</th>\n",
       "      <th>PROD_NBR</th>\n",
       "      <th>PROD_NAME</th>\n",
       "      <th>PROD_QTY</th>\n",
       "      <th>TOT_SALES</th>\n",
       "      <th>WEIGHT_IN_GRAMS</th>\n",
       "      <th>PACK_SIZE</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>LIFESTAGE</th>\n",
       "      <th>PREMIUM_CUSTOMER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17-10-2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Natural Chip Compny SeaSalt175g</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>175</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Natural</td>\n",
       "      <td>YOUNG SINGLES/COUPLES</td>\n",
       "      <td>Premium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14-05-2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1307</td>\n",
       "      <td>348</td>\n",
       "      <td>66</td>\n",
       "      <td>CCs Nacho Cheese 175g</td>\n",
       "      <td>3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>175</td>\n",
       "      <td>Medium</td>\n",
       "      <td>CCs</td>\n",
       "      <td>MIDAGE SINGLES/COUPLES</td>\n",
       "      <td>Budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20-05-2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1343</td>\n",
       "      <td>383</td>\n",
       "      <td>61</td>\n",
       "      <td>Smiths Crinkle Cut Chips Chicken 170g</td>\n",
       "      <td>2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>170</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Smiths</td>\n",
       "      <td>MIDAGE SINGLES/COUPLES</td>\n",
       "      <td>Budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17-08-2018</td>\n",
       "      <td>2</td>\n",
       "      <td>2373</td>\n",
       "      <td>974</td>\n",
       "      <td>69</td>\n",
       "      <td>Smiths Chip Thinly S/Cream&amp;Onion 175g</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>175</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Smiths</td>\n",
       "      <td>MIDAGE SINGLES/COUPLES</td>\n",
       "      <td>Budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-08-2018</td>\n",
       "      <td>2</td>\n",
       "      <td>2426</td>\n",
       "      <td>1038</td>\n",
       "      <td>108</td>\n",
       "      <td>Kettle Tortilla ChpsHny&amp;Jlpno Chili 150g</td>\n",
       "      <td>3</td>\n",
       "      <td>13.8</td>\n",
       "      <td>150</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Kettle</td>\n",
       "      <td>MIDAGE SINGLES/COUPLES</td>\n",
       "      <td>Budget</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ADATE  STORE_NBR  LYLTY_CARD_NBR  TXN_ID  PROD_NBR  \\\n",
       "0  17-10-2018          1            1000       1         5   \n",
       "1  14-05-2019          1            1307     348        66   \n",
       "2  20-05-2019          1            1343     383        61   \n",
       "3  17-08-2018          2            2373     974        69   \n",
       "4  18-08-2018          2            2426    1038       108   \n",
       "\n",
       "                                  PROD_NAME  PROD_QTY  TOT_SALES  \\\n",
       "0           Natural Chip Compny SeaSalt175g         2        6.0   \n",
       "1                     CCs Nacho Cheese 175g         3        6.3   \n",
       "2     Smiths Crinkle Cut Chips Chicken 170g         2        2.9   \n",
       "3     Smiths Chip Thinly S/Cream&Onion 175g         5       15.0   \n",
       "4  Kettle Tortilla ChpsHny&Jlpno Chili 150g         3       13.8   \n",
       "\n",
       "   WEIGHT_IN_GRAMS PACK_SIZE    BRAND               LIFESTAGE PREMIUM_CUSTOMER  \n",
       "0              175    Medium  Natural   YOUNG SINGLES/COUPLES          Premium  \n",
       "1              175    Medium      CCs  MIDAGE SINGLES/COUPLES           Budget  \n",
       "2              170    Medium   Smiths  MIDAGE SINGLES/COUPLES           Budget  \n",
       "3              175    Medium   Smiths  MIDAGE SINGLES/COUPLES           Budget  \n",
       "4              150    Medium   Kettle  MIDAGE SINGLES/COUPLES           Budget  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "df = pd.read_csv('QVI_data_joined.csv')\n",
    "df['PREMIUM_CUSTOMER'] = df['PREMIUM_CUSTOMER'].str.strip()\n",
    "df['BRAND'] = df['BRAND'].str.strip()\n",
    "df['LIFESTAGE'] = df['LIFESTAGE'].str.strip()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29146c7a",
   "metadata": {},
   "source": [
    "#### Independent t-test to check statistical significance\n",
    "#### Mainstream vs Budget & Premium customer segments for these life-stages: Midage singles / couples , Young singles / couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b51c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PREMIUM_CUSTOMER               LIFESTAGE  TOT_SALES  PROD_QTY  \\\n",
      "0       Mainstream  MIDAGE SINGLES/COUPLES   90803.85     22699   \n",
      "1       Mainstream   YOUNG SINGLES/COUPLES  157621.60     38632   \n",
      "\n",
      "   AVG_PRICE_PER_UNIT  \n",
      "0            4.000346  \n",
      "1            4.080079  \n",
      "  PREMIUM_CUSTOMER               LIFESTAGE  TOT_SALES  PROD_QTY  \\\n",
      "0           Budget  MIDAGE SINGLES/COUPLES   35514.80      9496   \n",
      "1           Budget   YOUNG SINGLES/COUPLES   61141.60     16671   \n",
      "2          Premium  MIDAGE SINGLES/COUPLES   58432.65     15526   \n",
      "3          Premium   YOUNG SINGLES/COUPLES   41642.10     11331   \n",
      "\n",
      "   AVG_PRICE_PER_UNIT  \n",
      "0            3.739975  \n",
      "1            3.667542  \n",
      "2            3.763535  \n",
      "3            3.675060  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=np.float64(7.081639382806015), pvalue=np.float64(0.02681531869636337), df=np.float64(1.7629408734710912))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1 = df[(df['PREMIUM_CUSTOMER'] == 'Mainstream') & (df['LIFESTAGE'].isin(['MIDAGE SINGLES/COUPLES', 'YOUNG SINGLES/COUPLES']))].groupby(['PREMIUM_CUSTOMER','LIFESTAGE'], as_index=False).agg({'TOT_SALES':'sum','PROD_QTY':'sum'})\n",
    "group1['AVG_PRICE_PER_UNIT'] = group1['TOT_SALES'] / group1['PROD_QTY']\n",
    "print(group1)\n",
    "grp1ppu = group1['AVG_PRICE_PER_UNIT']\n",
    "\n",
    "group2 = df[(df['PREMIUM_CUSTOMER'].isin(['Budget','Premium'])) & (df['LIFESTAGE'].isin(['MIDAGE SINGLES/COUPLES', 'YOUNG SINGLES/COUPLES']))].groupby(['PREMIUM_CUSTOMER','LIFESTAGE'], as_index=False).agg({'TOT_SALES':'sum','PROD_QTY':'sum'})\n",
    "group2['AVG_PRICE_PER_UNIT'] = group2['TOT_SALES'] / group2['PROD_QTY']\n",
    "print(group2)\n",
    "grp2ppu = group2['AVG_PRICE_PER_UNIT']\n",
    "\n",
    "stats.ttest_ind(grp1ppu,grp2ppu,equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8115a7",
   "metadata": {},
   "source": [
    "We got a p value < 0.05 So, we can conclude that the avg. price per unit for mainstream segment is significantly higher than budget and premium segments for the following life-stages: midage singles/couples & young singles/couples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d3438",
   "metadata": {},
   "source": [
    "#### We can target groups that are a significant contributor to sales like Mainstream- young singles/couples in order to retain them. Letâ€™s find out if this group tend to buy a particular brand of chips using affinity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbad6eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           BRAND  targetSegment     other  affinityToBrand\n",
      "20      Tyrrells       0.029587  0.023933         1.236230\n",
      "19      Twisties       0.043306  0.035283         1.227396\n",
      "9         Kettle       0.185649  0.154217         1.203818\n",
      "18      Tostitos       0.042581  0.035377         1.203633\n",
      "12   Old El Paso       0.041598  0.034753         1.196953\n",
      "13      Pringles       0.111980  0.093744         1.194531\n",
      "5        Doritos       0.122877  0.105278         1.167171\n",
      "4           Cobs       0.041856  0.036375         1.150696\n",
      "8      Infuzions       0.060649  0.053157         1.140942\n",
      "17         Thins       0.056611  0.053084         1.066440\n",
      "7    Grain Waves       0.030674  0.029052         1.055821\n",
      "3       Cheezels       0.016851  0.017370         0.970137\n",
      "15        Smiths       0.093420  0.121710         0.767559\n",
      "6         French       0.003702  0.005364         0.690110\n",
      "2        Cheetos       0.007533  0.011240         0.670143\n",
      "14           RRD       0.045377  0.068427         0.663146\n",
      "11       Natural       0.014962  0.023270         0.642955\n",
      "10           NCC       0.003417  0.005471         0.624534\n",
      "1            CCs       0.010484  0.017602         0.595596\n",
      "16      Sunbites       0.005954  0.011719         0.508041\n",
      "21    Woolworths       0.028189  0.057429         0.490852\n",
      "0   Burger Rings       0.002744  0.006145         0.446535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter the data for the target segment\n",
    "segment1 = df[(df[\"LIFESTAGE\"] == \"YOUNG SINGLES/COUPLES\") & (df[\"PREMIUM_CUSTOMER\"] == \"Mainstream\")]\n",
    "other = df[~((df[\"LIFESTAGE\"] == \"YOUNG SINGLES/COUPLES\") & (df[\"PREMIUM_CUSTOMER\"] == \"Mainstream\"))]\n",
    "\n",
    "# Check unique values in both columns\n",
    "\n",
    "# Calculate total quantity for each segment\n",
    "quantity_segment1 = segment1[\"PROD_QTY\"].sum()\n",
    "quantity_other = other[\"PROD_QTY\"].sum()\n",
    "\n",
    "# Calculate brand proportions for both groups\n",
    "quantity_segment1_by_brand = segment1.groupby(\"BRAND\")[\"PROD_QTY\"].sum().reset_index()\n",
    "quantity_segment1_by_brand[\"targetSegment\"] = quantity_segment1_by_brand[\"PROD_QTY\"] / quantity_segment1\n",
    "quantity_segment1_by_brand.drop(columns=[\"PROD_QTY\"], inplace=True)\n",
    "\n",
    "quantity_other_by_brand = other.groupby(\"BRAND\")[\"PROD_QTY\"].sum().reset_index()\n",
    "quantity_other_by_brand[\"other\"] = quantity_other_by_brand[\"PROD_QTY\"] / quantity_other\n",
    "quantity_other_by_brand.drop(columns=[\"PROD_QTY\"], inplace=True)\n",
    "\n",
    "# Merge the two dataframes and calculate brand affinity\n",
    "brand_proportions = pd.merge(quantity_segment1_by_brand, quantity_other_by_brand, on=\"BRAND\")\n",
    "brand_proportions[\"affinityToBrand\"] = brand_proportions[\"targetSegment\"] / brand_proportions[\"other\"]\n",
    "\n",
    "# Sort by affinity in descending order\n",
    "brand_proportions = brand_proportions.sort_values(by=\"affinityToBrand\", ascending=False)\n",
    "\n",
    "print(brand_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801600b",
   "metadata": {},
   "source": [
    "We can see that :\n",
    "\n",
    "Mainstream young singles/couples are `23% more likely` to purchase `Tyrrells` chips compared to the\n",
    "rest of the population\n",
    "\n",
    "And they are `56% less likely` to purchase `Burger Rings` compared to the rest\n",
    "of the population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccfe81",
   "metadata": {},
   "source": [
    "#### Letâ€™s also find out if this group tends to buy larger packs of chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f80786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PACK_SIZE  targetSegment     other  affinityToPackSize\n",
      "0  Extra-Large       0.087622  0.070559            1.241815\n",
      "1        Large       0.128313  0.122092            1.050953\n",
      "2       Medium       0.775264  0.789740            0.981670\n",
      "3        Small       0.008801  0.017608            0.499824\n"
     ]
    }
   ],
   "source": [
    "# Calculate PACK_SIZE proportions for both groups\n",
    "quantity_segment1_by_packsize = segment1.groupby(\"PACK_SIZE\")[\"PROD_QTY\"].sum().reset_index()\n",
    "quantity_segment1_by_packsize[\"targetSegment\"] = quantity_segment1_by_packsize[\"PROD_QTY\"] / quantity_segment1\n",
    "quantity_segment1_by_packsize.drop(columns=[\"PROD_QTY\"], inplace=True)\n",
    "\n",
    "quantity_other_by_packsize = other.groupby(\"PACK_SIZE\")[\"PROD_QTY\"].sum().reset_index()\n",
    "quantity_other_by_packsize[\"other\"] = quantity_other_by_packsize[\"PROD_QTY\"] / quantity_other\n",
    "quantity_other_by_packsize.drop(columns=[\"PROD_QTY\"], inplace=True)\n",
    "\n",
    "# Merge the two dataframes and calculate affinity to PACK_SIZE\n",
    "packsize_proportions = pd.merge(quantity_segment1_by_packsize, quantity_other_by_packsize, on=\"PACK_SIZE\")\n",
    "packsize_proportions[\"affinityToPackSize\"] = packsize_proportions[\"targetSegment\"] / packsize_proportions[\"other\"]\n",
    "\n",
    "# Sort by affinity in descending order\n",
    "packsize_proportions = packsize_proportions.sort_values(by=\"affinityToPackSize\", ascending=False)\n",
    "\n",
    "print(packsize_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091736d",
   "metadata": {},
   "source": [
    "This group is `24% more likely` to purchase chips that have `extra-large` pack size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
